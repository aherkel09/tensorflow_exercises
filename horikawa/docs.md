The scripts (should) work as follows:
wordnet_reader.py searches subdirectories for tsv files & gets the wordnet offsets (numerical identifier for a word) from the 'category_id' column of each file. It then uses the wordnet interface to retrieve each word's 'lemma' (the actual word, e.g. 'bat', 'swan', &c.) & finally, writes the offsets, lemmas & definitions to a .csv file (offsets/imagery_offsets.csv attached)wordnet_by_run.py*** searches .tsv files for each run (I believe there are 20 runs of imagery dataper subject) & creates a separate file for the lemmas in each run (e.g. the wordnet offsets from run 1 are mapped to lemmas in tsv_out/lemmas-1.tsv)lemma_classifier.py goes through the lemma files for each run & asks the user to classify lemmas as living or nonliving. The user can then write these out (results in tsv_out/living_nonliving-XX.tsv), or view a list of their inputs.imagery_offsets.csv contains the output from wordnet_reader.py, i.e. all the unique lemmas from all the imagery runs. The version attached also has a living_nonliving column (1 --> living, 2 --> nonliving) & a count of the lemmas in each category.*** I wrote a script that takes the original .tsv files & splits them into columns (onset, duration, category_id, &c.) by run, but it must be on another machine somewhere. 
I've attached the output files from the missing script in the tsv_out subfolder.